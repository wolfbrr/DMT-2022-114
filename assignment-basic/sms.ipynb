{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e70f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03024f",
   "metadata": {},
   "source": [
    "## Load sms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d55986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_list = list()\n",
    "ham_list = list()\n",
    "def read_stopwords():\n",
    "    \"\"\"\n",
    "    Read all the stopwords from the file \"stopwords.txt\".\n",
    "    Returns the collection of stopwords.\n",
    "    \"\"\"\n",
    "    with open(\"stopwords.txt\") as f:\n",
    "        stopwords = [s.strip() for s in f.readlines()]\n",
    "\n",
    "    return set(stopwords)\n",
    "\n",
    "def convert_and_tokenze_string(s, tokens, stopwords):\n",
    "    \"\"\"\n",
    "    Strips a word from all puncuation, whitespace. Then converts\n",
    "    the word into all lower case.\n",
    "    \"\"\"\n",
    "    #remove new lines and spaces behind and infront of the string\n",
    "    for word in s.split():\n",
    "        word = word.strip(string.punctuation + \\\n",
    "            string.whitespace + \\\n",
    "            \"”\" + \"“\" + \"‘\" + \"’\" + \"―\" + \"—\" + \\\n",
    "            string.digits).lower()\n",
    "        if len(word)>0 and word not in stopwords:\n",
    "            if word in tokens:\n",
    "                tokens[word]+=1\n",
    "            else:\n",
    "                tokens[word]=1\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "with open('SmsCollection.csv', encoding='utf-8',  errors='ignore') as email_file:\n",
    "    for line in email_file:\n",
    "        if line.startswith(\"spam;\"):\n",
    "            subject = line.lstrip(\"spam;\")\n",
    "            spam_list.append(subject)\n",
    "        elif line.startswith(\"ham;\"):\n",
    "            subject = line.lstrip(\"ham;\")\n",
    "            ham_list.append(subject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9e0c5",
   "metadata": {},
   "source": [
    "### Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5245a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spam 747, number of ham 4827, total 5574\n"
     ]
    }
   ],
   "source": [
    "stopwords = read_stopwords()\n",
    "\n",
    "number_of_spam = len(spam_list)\n",
    "number_of_ham = len(ham_list)\n",
    "number_total = len(ham_list)+len(spam_list)\n",
    "print(f'number of spam {number_of_spam}, number of ham {number_of_ham}, total {number_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171d2b7",
   "metadata": {},
   "source": [
    "### Building bag of words for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387aff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train, spam_test = train_test_split(spam_list, test_size=0.2)\n",
    "ham_train, ham_test = train_test_split(ham_list, test_size=0.03)\n",
    "\n",
    "spam_train_bow = dict()\n",
    "ham_train_bow = dict()\n",
    "\n",
    "for subject in spam_train:\n",
    "    spam_train_bow = convert_and_tokenze_string(subject, spam_train_bow, stopwords)\n",
    "for subject in ham_train:\n",
    "    ham_train_bow = convert_and_tokenze_string(subject, ham_train_bow, stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da4fba",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d776342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_likelihood(vocabulary, subject):\n",
    "    total_words_in_vocabulary = np.sum(np.array(list(vocabulary.values())))\n",
    "    likelihood = len(vocabulary) # chance of ham/spam\n",
    "    tokens = convert_and_tokenze_string(subject, dict(), stopwords)\n",
    "\n",
    "    for s in tokens:\n",
    "        likelihood *= (vocabulary.get(s, 0)/total_words_in_vocabulary)**tokens[s]\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "def bag_of_words_classifier(spam_bow, ham_bow, string):\n",
    "    spam_likelihood = calculate_likelihood(spam_bow, string)\n",
    "    ham_likelihood = calculate_likelihood(ham_bow, string)\n",
    "    if spam_likelihood>ham_likelihood:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "bag_of_words_classifier(spam_train_bow, ham_train_bow, spam_test[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857c1aa",
   "metadata": {},
   "source": [
    "### check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5887477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 150\n",
      "143 145\n",
      "0.7389830508474576\n"
     ]
    }
   ],
   "source": [
    "out_spam=0\n",
    "for test_spam_str in spam_test:\n",
    "    out_spam+= int('spam' == bag_of_words_classifier(spam_train_bow, ham_train_bow, test_spam_str))\n",
    "print(out_spam, len(spam_test))\n",
    "\n",
    "out_ham = 0\n",
    "for test_ham_str in ham_test:\n",
    "    #print(bag_of_words_classifier(spam_train_bow, ham_train_bow, test_ham_str))\n",
    "    out_ham+= int('ham' == bag_of_words_classifier(spam_train_bow, ham_train_bow, test_ham_str))\n",
    "\n",
    "print(out_ham, len(ham_test))\n",
    "print((out_spam+out_ham)/(len(spam_test) + len(ham_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23196980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8fe74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
