{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab796d9b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cec730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8423ac",
   "metadata": {},
   "source": [
    "### load training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730774e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if loaded train file exists\n",
    "try:\n",
    "    with open('data/train-data.pickle', 'rb') as handle:\n",
    "        train = pickle.load(handle)\n",
    "#if not load and save\n",
    "except:\n",
    "    !unzip data/training_set_VU_DM.csv.zip\n",
    "    train = pd.read_csv('training_set_VU_DM.csv')\n",
    "    !rm training_set_VU_DM.csv\n",
    "    with open('data/train-data.pickle', 'wb') as handle:\n",
    "        pickle.dump(train, handle)\n",
    "\n",
    "#if loaded test file exists\n",
    "try:\n",
    "    with open('data/test-data.pickle', 'rb') as handle:\n",
    "        test = pickle.load(handle)\n",
    "#if not load and save\n",
    "except:\n",
    "    !unzip data/test_set_VU_DM.csv.zip\n",
    "    test = pd.read_csv('test_set_VU_DM.csv')\n",
    "    !rm test_set_VU_DM.csv\n",
    "    with open('data/test-data.pickle', 'wb') as handle:\n",
    "        pickle.dump(test, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb12627",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b60d3e",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd35fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(test_data, property_predicted_score):\n",
    "    !mv submission.csv.zip prev.submission.zip\n",
    "    test_data['raiting'] = property_predicted_score\n",
    "    submition_data = test_data[['srch_id','prop_id','raiting']]\n",
    "    submition_data = submition_data.sort_values(by=['srch_id', 'raiting'], ascending=[True,  False])\n",
    "    submition_data = submition_data.drop(columns=\"raiting\")\n",
    "    submition_data.to_csv('submission.csv', index=False)\n",
    "    !zip submission.csv.zip  submission.csv\n",
    "    !rm submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac4a50",
   "metadata": {},
   "source": [
    "#### ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5299dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg_@5 0.5\n"
     ]
    }
   ],
   "source": [
    "def discountedCumulativeGain(result, k=5):\n",
    "    \"\"\"\n",
    "    Evaluated per query\n",
    "    taken from \n",
    "    https://towardsdatascience.com/normalized-discounted-cumulative-gain-37e6f75090e9\n",
    "    \"\"\"\n",
    "    dcg = []\n",
    "    for idx, val in enumerate(result[0:k]): \n",
    "        #numerator = (2**val) - 1\n",
    "        numerator = val\n",
    "        # add 2 because python 0-index\n",
    "        denominator =  np.log2(idx + 2) \n",
    "        score = numerator/denominator\n",
    "        dcg.append(score)\n",
    "    return sum(dcg)\n",
    "\n",
    "\n",
    "def NDCG_at_k(X, ranking, ideal_ranking, k=5, verbose=True):\n",
    "    #create df with a querry rating and ideal rating\n",
    "    #df = X[['srch_id']]\n",
    "    #df.loc[: ,('ranking')] = ranking\n",
    "    #df.loc[: ,('true_ranking')] = ideal_ranking\n",
    "\n",
    "    d = {'srch_id': X['srch_id'], 'ranking': ranking, 'true_ranking': ideal_ranking}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df = df.sort_values(by=['srch_id', 'ranking'], ascending=[True,  False])\n",
    "\n",
    "    NDCG  = df.groupby('srch_id').aggregate\\\n",
    "    (ndcg=(\"true_ranking\", lambda x: discountedCumulativeGain(x, k)))\n",
    "\n",
    "    df = df.sort_values(by=['srch_id', 'true_ranking'], ascending=[True,  False])\n",
    "    INDCG  = df.groupby('srch_id').aggregate\\\n",
    "    (indcg=(\"true_ranking\", lambda x: discountedCumulativeGain(x, k)))\n",
    "    INDCG = INDCG[INDCG['indcg']!=0]\n",
    "    x = NDCG['ndcg']/INDCG['indcg']# true ranking has the information regarding actual booking\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'ndcg_@{k} {x.mean()}')\n",
    "\n",
    "    return x.mean()\n",
    "\n",
    "d = {'srch_id': [1, 1, 1, 1, 1, 1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "ndcg_score = NDCG_at_k(df, ranking=[5, 0, 0, 0, 0, 0], ideal_ranking=[0, 0, 5, 0, 0, 0], k=5)\n",
    "\n",
    "#assert discountedCumulativeGain([0,0,5], k=5)==((2**5-1)/np.log2(4) ) , \"assertion discountedCumulativeGain wrong\"\n",
    "assert discountedCumulativeGain([0,0,5], k=5)==(5/np.log2(4) ) , \"assertion discountedCumulativeGain wrong\"\n",
    "assert ndcg_score == (5/np.log2(4))/5 , \"assertion ndcg wrong\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c746f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_validation(model, X, n_splits=5):\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    ndcg_score = 0\n",
    "    for train, test in kfold.split(X):\n",
    "        X_train, X_test = X.iloc[train, :], X.iloc[test, :]\n",
    "        model.fit(X_train[features_to_choose], target_function(X_train))\n",
    "        Y_evaluation = model.predict(X_test[features_to_choose])\n",
    "        Y_ideal = target_function(X_test)\n",
    "        ndcg_score += NDCG_at_k(X_test, ranking=Y_evaluation, ideal_ranking=Y_ideal, k=5, verbose=False)\n",
    "    average = ndcg_score/n_splits\n",
    "    print(average)\n",
    "    return average\n",
    "\n",
    "def target_function(data_frame):\n",
    "    return data_frame['click_bool'] + data_frame['booking_bool']*4 #if booked then clicked booked = 5\n",
    "\n",
    "def hotel_booking_likelihood(data_frame, df_out):\n",
    "    lklhd = data_frame.groupby('prop_id').sum().reset_index()\n",
    "    counts = data_frame.groupby('prop_id').size().reset_index(name='counts')\n",
    "    lklhd = lklhd[['prop_id','booking_bool']]\n",
    "    lklhd = lklhd.rename(columns={\"booking_bool\": \"lklhd\"})\n",
    "    lklhd['lklhd'] = lklhd['lklhd']/counts['counts']\n",
    "    data_frame = data_frame.merge(lklhd, left_on='prop_id', right_on='prop_id')\n",
    "    df_out = df_out.merge(lklhd, left_on='prop_id', right_on='prop_id')\n",
    "    df_out.loc[df_out['lklhd'].isnull(),'lklhd'] = 0\n",
    "\n",
    "    return data_frame, df_out\n",
    "\n",
    "def features_engeneering(data_frame):\n",
    "    #fill missing property review score by median over whole data\n",
    "    for i in ['prop_review_score', 'prop_location_score2', 'orig_destination_distance']:\n",
    "        data_frame.loc[data_frame[i].isnull(),i] = data_frame[i].median()\n",
    "\n",
    "    #for i in ['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']:\n",
    "    #    data_frame.loc[data_frame[i].isnull(),i] = 0\n",
    "\n",
    "    #data_frame.loc[:, 'comp'] = \\\n",
    "    #data_frame[['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', \\\n",
    "    #             'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']].sum(axis=1)\n",
    "    \n",
    "    #mask = data_frame.loc[:, 'comp']>0\n",
    "    #data_frame.loc[mask, 'comp'] = 1\n",
    "    #mask = data_frame.loc[:, 'comp']<0\n",
    "    #data_frame.loc[mask, 'comp'] = -1\n",
    "\n",
    "    \n",
    "    #fill missing prop_log_historical_price  by country id median\n",
    "    mask_log_0 = data_frame['prop_log_historical_price'] == 0\n",
    "    data_frame[['historical_price']] = data_frame[['prop_log_historical_price']].applymap(np.exp)\n",
    "    data_frame.loc[mask_log_0, 'historical_price'] = data_frame.loc[mask_log_0, 'price_usd']\n",
    "\n",
    "    price_per_country_median = data_frame.groupby('prop_country_id').median().reset_index()\n",
    "    price_per_country_median = price_per_country_median[['prop_country_id','price_usd']]\n",
    "    price_per_country_median = price_per_country_median.rename(columns={\"price_usd\": \"price_per_country_median\"})\n",
    "    data_frame = data_frame.merge(price_per_country_median, left_on='prop_country_id', right_on='prop_country_id')\n",
    "    # apply median to missing values in price_usd\n",
    "    mask = data_frame['price_usd']==0\n",
    "    data_frame.loc[mask, 'price_usd'] = data_frame.loc[mask, 'price_per_country_median']\n",
    "    mask = data_frame['historical_price']==0\n",
    "    data_frame.loc[mask, 'historical_price'] = data_frame.loc[mask, 'price_per_country_median']\n",
    "\n",
    "    # normalization\n",
    "    #data_frame['price_usd'] = data_frame['price_usd']/data_frame['price_per_country_median']\n",
    "    #data_frame['historical_price'] = data_frame['historical_price']/data_frame['price_per_country_median']\n",
    "\n",
    "    # normalise by number of persons\n",
    "    number_of_person = (data_frame['srch_adults_count'] + data_frame['srch_children_count']/2)\n",
    "    data_frame['price_per_person_per_night'] = data_frame['price_usd']/number_of_person/data_frame['srch_length_of_stay']\n",
    "        \n",
    "    #\n",
    "    #data standartization\n",
    "    \n",
    "    #for feature in ['prop_starrating','prop_location_score1']:\n",
    "    #    x = data_frame[feature].values\n",
    "    #    x = x.reshape(-1,1)\n",
    "    #    data_frame[feature] = normalize(x)\n",
    "    \n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "def undersample(data_frame):\n",
    "    mask = data_frame['click_bool']==False\n",
    "    average_non_booked = data_frame[mask].groupby('srch_id').mean().reset_index()\n",
    "    \n",
    "    data_frame = data_frame.drop(data_frame[mask].index)\n",
    "    data_frame = data_frame.reset_index()\n",
    "\n",
    "    data_frame = data_frame.append(average_non_booked)\n",
    "    data_frame = data_frame.sort_values(by=['srch_id'], ascending=[True])\n",
    "    \n",
    "    #Fixing boolean values\n",
    "    for feature in ['random_bool', 'prop_brand_bool','promotion_flag','srch_saturday_night_bool' ]:\n",
    "        data_frame.loc[data_frame[feature]<0.5, feature] = False\n",
    "        data_frame.loc[data_frame[feature]>=0.5, feature] = True\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6c0eb",
   "metadata": {},
   "source": [
    "### Total columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474a3f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4958347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0521ba",
   "metadata": {},
   "source": [
    "### Total null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367f65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srch_id                              0\n",
      "date_time                            0\n",
      "site_id                              0\n",
      "visitor_location_country_id          0\n",
      "visitor_hist_starrating        4706481\n",
      "visitor_hist_adr_usd           4705359\n",
      "prop_country_id                      0\n",
      "prop_id                              0\n",
      "prop_starrating                      0\n",
      "prop_review_score                 7364\n",
      "prop_brand_bool                      0\n",
      "prop_location_score1                 0\n",
      "prop_location_score2           1090348\n",
      "prop_log_historical_price            0\n",
      "position                             0\n",
      "price_usd                            0\n",
      "promotion_flag                       0\n",
      "srch_destination_id                  0\n",
      "srch_length_of_stay                  0\n",
      "srch_booking_window                  0\n",
      "srch_adults_count                    0\n",
      "srch_children_count                  0\n",
      "srch_room_count                      0\n",
      "srch_saturday_night_bool             0\n",
      "srch_query_affinity_score      4640941\n",
      "orig_destination_distance      1607782\n",
      "random_bool                          0\n",
      "comp1_rate                     4838417\n",
      "comp1_inv                      4828788\n",
      "comp1_rate_percent_diff        4863908\n",
      "comp2_rate                     2933675\n",
      "comp2_inv                      2828078\n",
      "comp2_rate_percent_diff        4402109\n",
      "comp3_rate                     3424059\n",
      "comp3_inv                      3307357\n",
      "comp3_rate_percent_diff        4485550\n",
      "comp4_rate                     4650969\n",
      "comp4_inv                      4614684\n",
      "comp4_rate_percent_diff        4827261\n",
      "comp5_rate                     2735974\n",
      "comp5_inv                      2598327\n",
      "comp5_rate_percent_diff        4117248\n",
      "comp6_rate                     4718190\n",
      "comp6_inv                      4697371\n",
      "comp6_rate_percent_diff        4862173\n",
      "comp7_rate                     4642999\n",
      "comp7_inv                      4601925\n",
      "comp7_rate_percent_diff        4819832\n",
      "comp8_rate                     3041693\n",
      "comp8_inv                      2970844\n",
      "comp8_rate_percent_diff        4343617\n",
      "click_bool                           0\n",
      "gross_bookings_usd             4819957\n",
      "booking_bool                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17cda5",
   "metadata": {},
   "source": [
    "### Amount of hotels in data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1114ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of hotels in test data 129438 \n",
      "Amount of hotels in train data 129113\n",
      "Amount of uniqe hotels in train and not in test 7773\n"
     ]
    }
   ],
   "source": [
    "x = test['prop_id']\n",
    "print(f'Amount of hotels in test data {len(set(x))} ')\n",
    "y = train['prop_id']\n",
    "print(f'Amount of hotels in train data {len(set(y))}')\n",
    "print(f'Amount of uniqe hotels in train and not in test {len(set(x) - set(y))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89a0090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns not in both sets {'position', 'booking_bool', 'click_bool'}\n"
     ]
    }
   ],
   "source": [
    "train_ = features_engeneering(train)\n",
    "test_ = features_engeneering(test)\n",
    "\n",
    "df_ = train_.dropna(axis=1)\n",
    "df_, test = hotel_booking_likelihood(df_, test_)\n",
    "df = df_.sort_values(by=['srch_id','prop_id'])\n",
    "\n",
    "#df, test = (df_, test_)\n",
    "\n",
    "a = set(df.columns)\n",
    "b = set(test.columns)\n",
    "print(f'columns not in both sets {a-b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbe0e5",
   "metadata": {},
   "source": [
    "### Choose features to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c055b3c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srch_id', 'date_time', 'site_id', 'visitor_location_country_id',\n",
      "       'prop_country_id', 'prop_id', 'prop_starrating', 'prop_review_score',\n",
      "       'prop_brand_bool', 'prop_location_score1', 'prop_location_score2',\n",
      "       'prop_log_historical_price', 'position', 'price_usd', 'promotion_flag',\n",
      "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
      "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
      "       'srch_saturday_night_bool', 'orig_destination_distance', 'random_bool',\n",
      "       'click_bool', 'booking_bool', 'historical_price',\n",
      "       'price_per_country_median', 'price_per_person_per_night', 'lklhd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)#  the columns after droping\n",
    "\n",
    "features_to_choose = ['visitor_location_country_id',\n",
    "                      'prop_country_id', \n",
    "                      'prop_id',  \n",
    "                      'prop_starrating',\n",
    "                      'prop_review_score',\n",
    "                      'prop_brand_bool', \n",
    "                      'prop_location_score1',\n",
    "                      'prop_log_historical_price', #historical_price\n",
    "                      'promotion_flag',                      \n",
    "                      'srch_destination_id',\n",
    "                      'srch_booking_window',\n",
    "                      'srch_saturday_night_bool', \n",
    "                      'random_bool', \n",
    "                      'prop_location_score2', \n",
    "                      'orig_destination_distance', \n",
    "                      #'comp', \n",
    "                      #'lklhd',\n",
    "                      'price_per_person_per_night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c8759dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.frequent_patterns import apriori\n",
    "#from mlxtend.frequent_patterns import association_rules\n",
    "#frequent_itemsets= apriori(df[['promotion_flag','srch_saturday_night_bool', 'random_bool', 'click_bool', 'booking_bool']], min_support=0.07, use_colnames=True)\n",
    "#rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "#rules = rules.sort_values(by=[ 'support', 'confidence',], ascending=False)\n",
    "\n",
    "#rules[['antecedents', 'consequents','support','confidence']].iloc[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f539a",
   "metadata": {},
   "source": [
    "### Test and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfdbbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2436082564616758\n",
      "The time of evaluation fit: 22.986633777618408\n",
      "0.25744320666850773\n",
      "The time of evaluation fit: 53.53840613365173\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Fit regression model\n",
    "#model = linear_model.LinearRegression()\n",
    "#n_neighbors = 10\n",
    "#model = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "#model= DecisionTreeRegressor(random_state=0)\n",
    "#model = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "########       Cross EVALUATION           ###############\n",
    "\n",
    "size = 0.05\n",
    "train_idx = int(len(df)*2/3)\n",
    "evaluation_size = int(train_idx*1/7)\n",
    "\n",
    "X_train = df.iloc[0:(train_idx-evaluation_size), :]\n",
    "X_eval = df.iloc[(train_idx-evaluation_size):train_idx, :]\n",
    "X_test = df.iloc[train_idx:, :]\n",
    "\n",
    "x = []\n",
    "depth = [x for x in range(1,15) if x % 2 != 0]\n",
    "\n",
    "for i in depth:\n",
    "    start = time.time()\n",
    "    model = RandomForestRegressor(max_depth=i, random_state=0, n_jobs=-1)\n",
    "    ndcg_score = my_cross_validation(model, X_eval, n_splits=5)\n",
    "    end = time.time()\n",
    "    print(\"The time of evaluation fit:\", end-start)\n",
    "    x.append(ndcg_score)\n",
    "    \n",
    "optimal_score_index = x.index(max(x))\n",
    "optimal_depth = depth[optimal_score_index]\n",
    "print(optimal_depth)# plot misclassification error vs k\n",
    "plt.plot(depth, x)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('ndcg_score')\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "ind = int(len(df)*0.5)\n",
    "\n",
    "start = time.time()\n",
    "model = RandomForestRegressor(max_depth=optimal_depth, random_state=0, n_jobs=-1)\n",
    "model.fit(X_train[features_to_choose], target_function(X_train))\n",
    "end = time.time()\n",
    "print(\"The time of evaluation fit:\", end-start)\n",
    "Y_pred = model.predict(X_test[features_to_choose])\n",
    "ndcg_score = NDCG_at_k(X_test, ranking=Y_pred, ideal_ranking= target_function(X_test), k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c14a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=0)\n",
    "model.fit(X_evalX_eval[features_to_choose], target_function(X_evalX_evalX_eval))\n",
    "Y_evaluation = model.predict(X_eval[features_to_choose])\n",
    "\n",
    "ndcg_score = NDCG_at_k(X_eval, ranking=Y_evaluation, ideal_ranking=target_function(X_eval), k=5)\n",
    "\n",
    "\n",
    "##\n",
    "ind = int(len(df)*0.5)\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "model.fit(X_train[features_to_choose], target_function(X_train))\n",
    "Y_pred = model.predict(X_test[features_to_choose])\n",
    "\n",
    "ndcg_score = NDCG_at_k(X_test, ranking=Y_pred, ideal_ranking=target_function(X_test), k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e51eb",
   "metadata": {},
   "source": [
    "### Fit and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "model.fit(df_[features_to_choose], target_function(df))\n",
    "end = time.time()\n",
    "print(\"The time of final fit:\", end-start)\n",
    "raitings = model.predict(test_[features_to_choose])\n",
    "create_submission(test_, raitings)\n",
    "\n",
    "start = time.time()\n",
    "model = RandomForestRegressor(max_depth=optimal_depth, random_state=0, n_jobs=-1)\n",
    "model.fit(df_[features_to_choose], target_function(df))\n",
    "end = time.time()\n",
    "print(\"The time of final fit:\", end-start)\n",
    "raitings = model.predict(test_[features_to_choose])\n",
    "create_submission(test_, raitings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
